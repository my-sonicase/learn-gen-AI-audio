{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"\ud83d\udd0a How to Learn AI for Audio &amp; Sound","text":"<p> Ciao! This is My Sonicase blog on how I learn AI for sound and audio. I believe the best way to actually learn AI is to build things. Follow a great course, and in the meantime go implement stuff. Read a paper, then try to reproduce it. Break things. That's how it sticks. So this is my open lab where I do exactly that. The twist is that everything I build, I apply to sound, music, and audio. Even when a course teaches general deep learning, I always steer my projects toward generative soundscapes, procedural audio, and creative sonic experiments. If it makes a sound, I'm happier and learn better. Let's go! </p> generative audio generative soundscapes procedural AI sound music generation gen AI audio synthesis neural audio sound design \u00d7 ML <p>Each tile links to a page where I document what I'm learning, the code I write, and the audio experiments I run. It's all a work in progress. That's the point.</p> <p>\ud83e\udde0 Foundational AI</p> \ud83e\uddea Foundational Karpathy \u00b7 Neural Networks: Zero to Hero <p>Build GPT from scratch, character by character. The concepts (tokenization, embeddings, self attention, autoregressive generation) are exactly what powers MusicGen and AudioLM. Where I can, I swap text for audio tokens.</p> \ud83d\ude80 Foundational fast.ai \u00b7 Practical Deep Learning <p>Top down, code first deep learning. I take every lesson and rebuild the exercises around audio spectrograms, waveform classification, and sound generation.</p> \ud83d\udcd0 Foundational Andrew Ng \u00b7 Deep Learning Specialization <p>The classic. Neural networks from scratch, CNNs, sequence models, all re applied to audio features, mel spectrograms, and music related tasks.</p> <p>\ud83c\udfa7 Applied AI for Audio</p> \ud83e\udd17 Applied \u00b7 Audio Hugging Face Audio Course <p>Hands on transformers for audio. Classification, generation, music tagging, working with audio datasets on the Hub.</p> <p>\ud83c\udfa8 Generative &amp; Creative AI</p> \ud83c\udf0a Generative Generative Audio Experiments <p>My sandbox for diffusion based sound, neural synthesis, generative soundscapes, and procedural audio. Projects I want to try, ideas I want to test.</p> <p>\ud83d\udcc4 Papers &amp; Research</p> \ud83d\udd2c Research Paper Implementations <p>Key papers I study and reproduce: AudioLDM, MusicGen, Riffusion, RAVE, SampleRNN, and others. Notes and code for each.</p> <p>How to navigate</p> <p>Click any tile to see my notes, code, and audio demos. Everything here is unfinished and evolving. That's how learning works.</p>"},{"location":"courses/deeplearning-ai.html","title":"\ud83d\udcd0 Andrew Ng \u00b7 Deep Learning Specialization","text":"<p> Andrew Ng's Deep Learning Specialization on Coursera is probably the single most important course for understanding neural networks from the ground up. Everyone should do it. My twist: every assignment and side project I do here uses audio data. Mel spectrograms as images, raw waveforms as sequences, music and environmental sound as classification targets. Same math, different domain. </p>"},{"location":"courses/deeplearning-ai.html#progress","title":"Progress","text":"Course Topic My Audio Project Status 1 Neural Networks &amp; Deep Learning Coming soon \ud83d\udd32 2 Improving Deep Neural Networks Coming soon \ud83d\udd32 3 Structuring ML Projects Coming soon \ud83d\udd32 4 CNNs Coming soon \ud83d\udd32 5 Sequence Models Coming soon \ud83d\udd32 <p>This page will grow as I work through the specialization.</p>"},{"location":"courses/fastai.html","title":"\ud83d\ude80 fast.ai \u00b7 Practical Deep Learning for Coders","text":"<p> fast.ai teaches deep learning top down. You build working models from day one, then progressively understand what's happening under the hood. Jeremy Howard's whole philosophy is \"just get coding\" and I love that. My twist: every exercise, every project, every experiment here is steered toward audio. Spectrograms, waveforms, sound classification, generative sound. If it makes noise, I'm in. </p>"},{"location":"courses/fastai.html#progress","title":"Progress","text":"Lesson Topic My Audio Project Status 1 Getting started Coming soon \ud83d\udd32 2 Deployment Coming soon \ud83d\udd32 3 Neural net foundations Coming soon \ud83d\udd32 <p>This page will grow as I work through the course.</p>"},{"location":"courses/gen-audio.html","title":"\ud83c\udf0a Generative Audio Experiments","text":"<p> This is where I collect experiments that don't belong to a single course. Diffusion based audio, neural synthesizers, generative soundscapes, procedural audio techniques. Sometimes inspired by a paper, sometimes just an idea I want to try. If it sounds interesting (literally), it ends up here. </p>"},{"location":"courses/gen-audio.html#experiments","title":"Experiments","text":"<p>Coming soon. First experiments in progress.</p> <p>This page will grow as I explore and build.</p>"},{"location":"courses/karpathy.html","title":"\ud83e\uddea Karpathy \u00b7 Neural Networks: Zero to Hero","text":"<p> Andrej Karpathy's Zero to Hero series builds neural networks from scratch, starting from a single neuron and ending with GPT. It's the best resource I know for truly understanding what's happening inside these models. The concepts (tokenization, embeddings, self attention, autoregressive generation) are exactly the same ones that power audio models like MusicGen, AudioLM, and SoundStream. My approach: where I can, I swap text for audio. Where I can't, I write notes on how each concept connects to the audio world. </p>"},{"location":"courses/karpathy.html#videos-audio-connections","title":"Videos &amp; Audio Connections","text":"# Video Core concept Audio connection Status 1 Micrograd Backprop, autograd engine Pure math. No direct audio application, but understanding gradients is essential for everything that follows. \ud83d\udd32 2 Makemore 1: Bigrams Character level language model Build a bigram model that predicts the next frame of a mel spectrogram instead of the next character. First taste of autoregressive audio. \ud83d\udd32 3 Makemore 2: MLP Multi layer perceptrons, embeddings Embeddings are how audio tokens (EnCodec, SoundStream) get represented as vectors. Same idea as character embeddings, different domain. \ud83d\udd32 4 Makemore 3: BatchNorm Activations, BatchNorm, diagnostics Training dynamics. Applies the same way to audio models. Good practice with training diagnostics. \ud83d\udd32 5 Makemore 4: Becoming a Backprop Ninja Manual backprop Pure math again. No audio twist needed, just do it and understand it. \ud83d\udd32 6 Makemore 5: WaveNet Dilated causal convolutions WaveNet was originally built for audio! This is where Karpathy's series and audio generation directly intersect. Implement it on actual waveforms. \ud83d\udd32 7 GPT from scratch Self attention, transformer, autoregressive generation This is the architecture behind MusicGen and AudioLM. Build GPT on text, then think about what changes when your tokens are audio codes instead of words. \ud83d\udd32 8 Tokenization BPE, tokenizers Directly connects to EnCodec/SoundStream: how do you turn continuous audio into discrete tokens? BPE on text \u2194 neural codec on audio. Same problem, different modality. \ud83d\udd32 9 Reproducing GPT-2 Full training pipeline at scale The full picture. Training infrastructure, data loading, optimization. Everything you'd need to train an audio GPT. \ud83d\udd32"},{"location":"courses/karpathy.html#which-videos-translate-best-to-audio","title":"Which videos translate best to audio?","text":"<p>Direct audio application (build something that makes sound): Makemore 1 (bigram on audio frames), Makemore 5/WaveNet (literally an audio model), GPT from scratch (the MusicGen architecture).</p> <p>Conceptual bridge (understand the idea, connect it to audio): Tokenization (BPE \u2194 neural audio codecs), Makemore 2 (embeddings \u2194 audio token embeddings).</p> <p>Pure foundations (just learn it, it applies everywhere): Micrograd, Makemore 3 and 4, Reproducing GPT-2.</p> <p>This page will grow as I work through the series.</p>"},{"location":"courses/hf-audio/index.html","title":"\ud83e\udd17 Hugging Face Audio Course","text":"<p> The Hugging Face Audio Course is a hands on guide to working with audio using the Hugging Face ecosystem. Loading datasets, fine tuning models for classification, generation, and everything in between. This one is directly about audio so I don't need to \"translate\" the exercises. I just go deep. </p>"},{"location":"courses/hf-audio/index.html#chapters","title":"Chapters","text":"Unit Topic Status 1 Working with Audio Data \u2705 Done 2 Audio Applications with Pipelines \u2705 Done 3 Inside Audio Transformers \u2705 Done 4 Fine Tuning Audio Classifiers \u2705 Done 5 Automatic Speech Recognition \ud83d\udd32 <p>This page will grow as I work through the course.</p>"},{"location":"courses/hf-audio/chapter1.html","title":"Chapter 1: Working with Audio Data","text":"<p> Following the Hugging Face Audio Course, Unit 1, but instead of speech I'm using soundscapes. Two sounds from the Free to Use Sounds library: thunder/rain/cicadas recorded in Ubud, Indonesia, and musical chimes recorded in Georgia. These have very different spectral signatures and that's what makes them fun to compare. </p> <p>\ud83d\udcd3 Full notebook on GitHub</p>"},{"location":"courses/hf-audio/chapter1.html#the-sounds","title":"The sounds","text":"<p>Two files, two very different worlds. Thunder is broadband noise, energy smeared everywhere. Chimes are tonal, with clear harmonic peaks. Before we even plot anything, just listen.</p>"},{"location":"courses/hf-audio/chapter1.html#original-vs-16-khz","title":"Original vs 16 kHz","text":"<p>The originals were recorded at 96 kHz (thunder) and 192 kHz (chimes). For the blog I've converted them to 48 kHz MP3, which is still way more than what ML models need. We resample to 16 kHz because that's what most audio models expect (Whisper, Wav2Vec2, etc). Listen to both and see if you can hear the difference:</p> <p>\ud83c\udf27\ufe0f Thunder / Rain / Cicadas, Ubud, Indonesia</p> <p>Original (48 kHz MP3):  </p> <p>Resampled (16 kHz):  </p> <p>\ud83d\udd14 Musical Chimes, Georgia</p> <p>Original (48 kHz MP3):  </p> <p>Resampled (16 kHz):  </p> <p>The 16 kHz versions sound a bit duller, especially in the high end. The chimes lose some of their shimmer. But for an ML model doing classification? More than enough information.</p>"},{"location":"courses/hf-audio/chapter1.html#loading-and-inspecting","title":"Loading and inspecting","text":"<pre><code>import librosa\n\n# Load at native sample rate first\nthunder_native, sr_thunder = librosa.load(\"thunder.wav\", sr=None, mono=True)\nchimes_native, sr_chimes = librosa.load(\"chimes.wav\", sr=None, mono=True)\n\n# Then resample to 16 kHz for ML\nSR = 16000\nthunder = librosa.resample(thunder_original, orig_sr=sr_thunder, target_sr=SR)\nchimes = librosa.resample(chimes_original, orig_sr=sr_chimes, target_sr=SR)\n</code></pre> <p>A few things about the raw files. 96 kHz and 192 kHz sample rates, stereo channels, PCM_24 and FLOAT encodings. These come from a professional sound library. <code>librosa.load()</code> handles all the conversion: mixes to mono, resamples, normalizes to float32. One line and you're done.</p>"},{"location":"courses/hf-audio/chapter1.html#what-is-sampling-rate-really","title":"What is sampling rate, really?","text":"<p>The sampling rate is how many times per second the continuous sound wave was measured. At 96,000 Hz, we captured 96,000 snapshots per second. The Nyquist theorem says this lets us faithfully represent frequencies up to half the sampling rate (48 kHz for 96 kHz SR). Human hearing tops out at ~20 kHz, so 96 kHz is overkill for most purposes. For ML models, 16 kHz is standard, enough to capture everything below 8 kHz.</p>"},{"location":"courses/hf-audio/chapter1.html#waveforms-the-time-domain","title":"Waveforms: the time domain","text":"<p>A waveform shows amplitude over time. Each point is one sample. It tells you when things happen and how loud they are, but nothing about which frequencies are present.</p> <pre><code>fig, axes = plt.subplots(2, 1, figsize=(14, 6))\nlibrosa.display.waveshow(thunder, sr=SR, ax=axes[0], color='steelblue')\nlibrosa.display.waveshow(chimes, sr=SR, ax=axes[1], color='coral')\n</code></pre> <p></p> <p>Thunder/rain has relatively constant amplitude with occasional spikes (thunder claps, heavy drips). The rain creates a continuous \"bed\" of sound. Chimes show clear attacks when each chime is struck, followed by decay as the tone fades out. The silence between strikes is visible. Already very different, but the waveform doesn't tell us what frequencies make up each sound.</p>"},{"location":"courses/hf-audio/chapter1.html#frequency-spectrum-fft","title":"Frequency spectrum (FFT)","text":"<p>The Discrete Fourier Transform converts a signal from time domain to frequency domain. It answers: which frequencies are present, and how strong are they? We use the FFT (Fast Fourier Transform), which is just an efficient algorithm for computing the DFT. We take a short window of each sound (4096 samples \u2248 0.25s at 16 kHz) and look at its spectrum.</p> <pre><code>def plot_spectrum(signal, sr, title, color='steelblue', n_fft=4096):\n    chunk = signal[:n_fft]\n    window = np.hanning(n_fft)  # smooths edges, avoids spectral leakage\n    spectrum = np.abs(np.fft.rfft(chunk * window))\n    freqs = np.fft.rfftfreq(n_fft, 1/sr)\n    plt.plot(freqs, 20 * np.log10(spectrum + 1e-10), color=color, linewidth=0.5)\n</code></pre> <p></p> <p>Thunder/rain: energy spread across many frequencies. This is what \"broadband noise\" looks like. Rain is essentially random fluctuations across the whole spectrum. Chimes: distinct peaks at specific frequencies, the fundamentals and harmonics of each chime. This is why frequency analysis is powerful. Two sounds that look similar as waveforms become clearly distinguishable in the frequency domain.</p> <p>But there's a problem: the FFT gives us frequencies for one fixed window of time. What if we want to see how frequencies change over time?</p>"},{"location":"courses/hf-audio/chapter1.html#spectrogram-stft","title":"Spectrogram (STFT)","text":"<p>A spectrogram is what you get when you compute many FFTs on overlapping windows across the whole signal, then stack them side by side. X axis is time, Y axis is frequency, color is amplitude. The algorithm is called the Short Time Fourier Transform (STFT).</p> <pre><code>D = librosa.stft(signal)\nS_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\nlibrosa.display.specshow(S_db, sr=SR, x_axis='time', y_axis='hz')\n</code></pre> <p></p> <p>Now we can see time AND frequency together. Thunder/rain: energy smeared across all frequencies over time, with bright vertical bands where thunder claps happen (sudden broadband energy). Chimes: horizontal lines (sustained tones at specific frequencies) that appear when a chime is struck and fade out. This is why spectrograms are the go to input for many audio ML models. They're basically \"images\" of sound.</p>"},{"location":"courses/hf-audio/chapter1.html#mel-spectrogram","title":"Mel spectrogram","text":"<p>This is what most ML models actually use. Whisper takes a log mel spectrogram as input. The idea: our ears don't perceive frequencies linearly. The difference between 100 Hz and 200 Hz sounds huge, but 8000 Hz vs 8100 Hz is barely noticeable. The mel scale warps the frequency axis to match human perception: more resolution at low frequencies, less at high frequencies.</p> <pre><code>S = librosa.feature.melspectrogram(y=signal, sr=SR, n_mels=128, fmax=8000)\nS_db = librosa.power_to_db(S, ref=np.max)\nlibrosa.display.specshow(S_db, sr=SR, x_axis='time', y_axis='mel', fmax=8000)\n</code></pre> <p></p> <p>The lower frequencies get more vertical space, which is where a lot of the interesting information lives.</p> Representation X axis Y axis Shows Used for Waveform Time Amplitude When things happen Quick inspection, editing FFT Spectrum Frequency Amplitude What frequencies exist Single point analysis Spectrogram Time Frequency (linear) Frequencies over time Visualization, analysis Mel Spectrogram Time Frequency (mel scale) Perceptually weighted frequencies ML model input"},{"location":"courses/hf-audio/chapter1.html#all-four-views-together","title":"All four views together","text":"<p>Here's everything side by side for each sound. Waveform, FFT spectrum, spectrogram, mel spectrogram.</p> <p></p> <p></p>"},{"location":"courses/hf-audio/chapter1.html#esc-50-an-environmental-sound-dataset","title":"ESC 50: an environmental sound dataset","text":"<p>The HF course uses MINDS 14 (a speech dataset). Since we're doing soundscapes, I use ESC 50 instead: 2,000 clips, 5 seconds each, 50 classes of environmental sounds. All originally from Freesound.org.</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"ashraq/esc50\", split=\"train\")\n</code></pre>"},{"location":"courses/hf-audio/chapter1.html#all-50-categories","title":"All 50 categories","text":"airplane breathing brushing_teeth can_opening car_horn cat chainsaw chirping_birds church_bells clapping clock_alarm clock_tick coughing cow crackling_fire crickets crow crying_baby dog door_wood_creaks door_wood_knock drinking_sipping engine fireworks footsteps frog glass_breaking hand_saw helicopter hen insects keyboard_typing laughing mouse_click pig pouring_water rain rooster sea_waves sheep siren sneezing snoring thunderstorm toilet_flush train vacuum_cleaner washing_machine water_drops wind <p>The bold ones are the categories closest to our own sounds. Let's listen to a few:</p> <p>\ud83c\udf27\ufe0f Rain (ESC 50) </p> <p>\u26c8\ufe0f Thunderstorm (ESC 50) </p> <p>\ud83c\udf0a Sea Waves (ESC 50) </p> <p>\ud83d\udd14 Church Bells (ESC 50) </p> <p>\ud83d\udca8 Wind (ESC 50) </p>"},{"location":"courses/hf-audio/chapter1.html#esc-50-rain-vs-our-thunder-recording","title":"ESC 50 rain vs our thunder recording","text":"<p>How does a 5 second ESC 50 rain clip compare to our own 27 second field recording? Let's look at the mel spectrograms side by side:</p> <p></p>"},{"location":"courses/hf-audio/chapter1.html#preprocessing-resampling-with-cast_column","title":"Preprocessing: resampling with cast_column","text":"<p>ESC 50 audio is at 44.1 kHz. Most models expect 16 kHz. Instead of downloading and resampling manually, HF Datasets can resample on the fly:</p> <pre><code>from datasets import Audio\n\ndataset_16k = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n</code></pre> <p><code>cast_column</code> doesn't resample everything at once. It happens only when you access each example. Important for large datasets where you don't want to process millions of files upfront.</p>"},{"location":"courses/hf-audio/chapter1.html#feature-extraction-with-whisper","title":"Feature extraction with Whisper","text":"<p>ML models don't accept raw audio arrays. They need input features, typically a log mel spectrogram with specific parameters. Each model has its own feature extractor that knows exactly how to transform the audio. Whisper expects 30 second chunks (padded if shorter), 80 mel bins, 16 kHz sample rate.</p> <pre><code>from transformers import WhisperFeatureExtractor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\ninputs = feature_extractor(thunder, sampling_rate=SR, return_tensors=\"np\")\n# Shape: (1, 80, 3000) \u2192 batch=1, mel_bins=80, time_frames=3000\n</code></pre> <p>This is literally what Whisper \"sees\":</p> <p></p> <p>80 mel bins, 3000 time frames (30 seconds). Shorter audio is zero padded on the right. You can apply this to a whole dataset with <code>map</code>:</p> <pre><code>def prepare_dataset(example):\n    audio = example[\"audio\"]\n    features = feature_extractor(\n        audio[\"array\"],\n        sampling_rate=audio[\"sampling_rate\"],\n        padding=True\n    )\n    return features\n\ndataset_processed = dataset_16k.map(prepare_dataset)\n</code></pre> <p>For models like Whisper that handle both audio input and text output, HF provides an <code>AutoProcessor</code> that bundles the feature extractor with a tokenizer:</p> <pre><code>from transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n# processor.feature_extractor \u2192 WhisperFeatureExtractor\n# processor.tokenizer \u2192 WhisperTokenizer\n</code></pre>"},{"location":"courses/hf-audio/chapter1.html#streaming","title":"Streaming","text":"<p>Audio datasets can be enormous. GigaSpeech is over 1 TB. Streaming mode loads examples one at a time, on the fly, no disk space needed. The trade off: you can't index by position (<code>dataset[42]</code>), you can only iterate.</p> <pre><code>esc50_stream = load_dataset(\"ashraq/esc50\", split=\"train\", streaming=True)\n\n# Can't do esc50_stream[0], must iterate\nfirst = next(iter(esc50_stream))\n\n# take() grabs the first N examples\nfor example in esc50_stream.take(5):\n    print(example['category'], example['audio']['sampling_rate'])\n\n# Resample streamed data on the fly too\nesc50_stream_16k = esc50_stream.cast_column(\"audio\", Audio(sampling_rate=16000))\n</code></pre> Situation Use streaming? Dataset fits on disk, you'll reuse it \u274c Download once Dataset is huge (100+ GB) \u2705 Stream Quick experiment, just need a few examples \u2705 Stream Evaluating on many datasets sequentially \u2705 Stream <p>For ESC 50 (~600 MB), streaming is optional. For AudioSet (~2M clips), it's essential.</p>"},{"location":"courses/hf-audio/chapter1.html#what-i-learned","title":"What I learned","text":"<p>The key insight from comparing thunder/rain and chimes: they look very different in the frequency domain. Broadband noise vs clear tonal peaks. This is exactly what ML models exploit for classification. The mel spectrogram is the most common input representation because it captures frequency information on a perceptually meaningful scale.</p> <p>The whole pipeline goes: raw audio \u2192 resample to 16 kHz \u2192 feature extractor \u2192 log mel spectrogram \u2192 model. Understanding each step makes everything that comes next in the course make more sense.</p> <p>\ud83d\udcd3 Full notebook with all the code</p>"},{"location":"courses/hf-audio/chapter2.html","title":"Chapter 2: Audio Applications with Pipelines","text":"<p> Following the Hugging Face Audio Course, Unit 2. The course shows three tasks: audio classification, ASR, and audio generation. I adapt all three to environmental sounds and soundscapes instead of speech. The key idea of this chapter: you can do a lot with pre-trained models and zero training. </p> <p>\ud83d\udcd3 Full notebook on GitHub</p> HF Course My version Audio Classification on MINDS 14 (speech intent) Audio Classification on ESC 50 (environmental sounds) ASR with Whisper (speech to text) Audio Captioning (sound to description) TTS with Bark + Music with MusicGen Soundscape generation with MusicGen"},{"location":"courses/hf-audio/chapter2.html#part-1-audio-classification","title":"Part 1: Audio Classification","text":"<p>Audio classification = give the model an audio clip, get back a label (or a ranked list of labels with scores). The HF course uses a model for speech intent classification. We use the Audio Spectrogram Transformer (AST) by MIT, trained on AudioSet (527 sound event classes).</p> <pre><code>from transformers import pipeline\n\nclassifier = pipeline(\n    \"audio-classification\",\n    model=\"MIT/ast-finetuned-audioset-10-10-0.4593\",\n)\n</code></pre>"},{"location":"courses/hf-audio/chapter2.html#classify-esc-50-examples","title":"Classify ESC 50 examples","text":"<p>Let's test it on sounds from the ESC 50 dataset. The model was trained on AudioSet which has different label names, so they won't match exactly. But the predictions should be semantically close.</p> <pre><code>test_categories = [\"rain\", \"thunderstorm\", \"church_bells\", \"sea_waves\", \"dog\"]\n\nfor cat in test_categories:\n    example = dataset.filter(lambda x: x[\"category\"] == cat)[0]\n    audio_array = np.array(example[\"audio\"][\"array\"], dtype=np.float32)\n    result = classifier(audio_array)\n</code></pre> True label Top 1 prediction Score Top 2 Score Top 3 Score rain Rain 0.396 Rain on surface 0.352 Raindrop 0.191 thunderstorm Thunder 0.548 Thunderstorm 0.254 Rain 0.109 church_bells Church bell 0.721 Bell 0.234 Change ringing 0.024 sea_waves Waves, surf 0.415 Ocean 0.310 Wind 0.058 dog Bark 0.192 Animal 0.184 Dog 0.108 <p>Pretty solid. The model has never seen ESC 50 data, yet it nails the semantics. Church bells is the most confident prediction. Dog is the weakest, probably because AudioSet has many fine grained animal categories.</p>"},{"location":"courses/hf-audio/chapter2.html#classify-our-own-sounds","title":"Classify our own sounds","text":"<p>Now our thunder and chimes recordings. The model has never seen these specific files.</p> Sound Top 1 Score Top 2 Score Top 3 Score Thunder/Rain Thunder 0.446 Thunderstorm 0.424 Rain 0.118 Chimes Wind chime 0.561 Chime 0.390 Tubular bells 0.016 <p>Both nailed. For thunder it can't decide between Thunder and Thunderstorm (makes sense, the recording has both). For chimes, Wind chime is the top prediction, which is exactly right.</p>"},{"location":"courses/hf-audio/chapter2.html#under-the-hood","title":"Under the hood","text":"<p>The <code>pipeline()</code> is convenient but hides what's happening. Here's the manual version:</p> <pre><code>from transformers import AutoFeatureExtractor, ASTForAudioClassification\n\nmodel_id = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\nmodel = ASTForAudioClassification.from_pretrained(model_id)\n\n# Preprocess\nclip = thunder[:SR * 10]\ninputs = feature_extractor(clip, sampling_rate=SR, return_tensors=\"pt\")\n# Input shape: (1, 1024, 128)\n\n# Inference\nwith torch.no_grad():\n    logits = model(**inputs).logits\n    probs = torch.softmax(logits, dim=-1)\n</code></pre> <p>That's exactly what <code>pipeline()</code> does in one line. Good to understand, but use the pipeline in practice.</p>"},{"location":"courses/hf-audio/chapter2.html#part-2-audio-captioning-instead-of-asr","title":"Part 2: Audio Captioning (instead of ASR)","text":"<p>The HF course uses Whisper for ASR (speech to text). Since our sounds are not speech, we use a Whisper model fine tuned for audio captioning: it generates a free text description of what it hears.</p> <p>The model (<code>MU-NLPC/whisper-tiny-audio-captioning</code>) is a standard Whisper encoder decoder fine tuned on audio captioning data. The authors provide a custom model class, but it breaks with recent <code>transformers</code> versions. No problem: the weights are identical, so we load them with <code>WhisperForConditionalGeneration</code> and handle the style prefix ourselves.</p> <p>This is a useful lesson: when a custom class breaks, understand what it does and replicate it with standard tools.</p> <pre><code>from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor\n\ncheckpoint = \"MU-NLPC/whisper-tiny-audio-captioning\"\ncaptioning_model = WhisperForConditionalGeneration.from_pretrained(checkpoint)\ntokenizer = WhisperTokenizer.from_pretrained(checkpoint)\ncap_feature_extractor = WhisperFeatureExtractor.from_pretrained(checkpoint)\n</code></pre> <p>The model supports 3 captioning styles via a text prefix:</p> <pre><code>def caption_audio(audio_array, sr, style=\"clotho &gt; caption: \"):\n    features = cap_feature_extractor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n    prefix_ids = tokenizer(style, return_tensors=\"pt\").input_ids\n    generated = captioning_model.generate(\n        features.input_features,\n        decoder_input_ids=prefix_ids\n    )\n    return tokenizer.decode(generated[0], skip_special_tokens=True)\n</code></pre>"},{"location":"courses/hf-audio/chapter2.html#caption-our-own-sounds","title":"Caption our own sounds","text":"Sound Style Caption Thunder/Rain Natural (Clotho) A man is walking down the street with thunder in his hands Thunder/Rain Short (AudioCaps) Rain falling and th falling Thunder/Rain Keywords (AudioSet) natural, fire, natural, natural Chimes Natural (Clotho) A person is tapping a glass glass in a room Chimes Short (AudioCaps) A series of musical tones playing in a musical instrument Chimes Keywords (AudioSet) onomatopoeia, jingle, alarm <p>The natural caption for thunder is hilariously wrong (\"a man walking down the street with thunder in his hands\") but the short caption and keywords get the gist. This is the tiny model (39M parameters). Larger variants produce significantly better captions but don't fit in Colab free tier.</p>"},{"location":"courses/hf-audio/chapter2.html#caption-esc-50-examples","title":"Caption ESC 50 examples","text":"True label Generated caption rain A large volume of water splashes as it flows thunderstorm Thunder rumbling in the distance sea_waves A large body of water church_bells Bells ringing in a church dog A dog barking <p>Much better on the clean 5 second clips. The thunderstorm and church bells captions are surprisingly good for a tiny model.</p>"},{"location":"courses/hf-audio/chapter2.html#asr-vs-audio-captioning","title":"ASR vs Audio Captioning","text":"ASR (Speech to Text) Audio Captioning Input Speech audio Any audio Output Exact transcription of words Free text description of sounds Architecture Encoder decoder (Whisper) Same encoder decoder (fine tuned Whisper) Training data Speech + transcription pairs Audio + description pairs Use case Subtitles, dictation, voice assistants Accessibility, search, metadata generation <p>Same architecture, different training data, completely different task. That's the power of transfer learning.</p>"},{"location":"courses/hf-audio/chapter2.html#part-3-audio-generation-with-musicgen","title":"Part 3: Audio Generation with MusicGen","text":"<p>The HF course shows text to speech with Bark and music generation with MusicGen. Since we're doing soundscapes, we use MusicGen to generate environmental audio from text prompts.</p> <pre><code>music_pipe = pipeline(\n    \"text-to-audio\",\n    model=\"facebook/musicgen-small\",\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n</code></pre>"},{"location":"courses/hf-audio/chapter2.html#generate-soundscapes-from-text","title":"Generate soundscapes from text","text":"<p>Three prompts, three generated sounds:</p> <p>\"Soft rain falling in a tropical forest with distant thunder and crickets\" </p> <p>\"Gentle wind chimes ringing with different tones in a peaceful garden\" </p> <p>\"Ocean waves crashing on a rocky shore with seagulls\" </p> <p>MusicGen is primarily a music model, so it tends to interpret environmental prompts as \"music inspired by\" rather than literal soundscapes. Still interesting to hear how it translates text to audio.</p>"},{"location":"courses/hf-audio/chapter2.html#real-recording-vs-generated","title":"Real recording vs generated","text":"<p>How does our real thunder/rain field recording compare to what MusicGen generates from a similar prompt?</p> <p>Real thunder/rain (first 10s): </p> <p>Generated: \"Soft rain falling in a tropical forest with distant thunder\" </p> <p>You can hear the difference immediately. The real recording has all the messy complexity of nature: layered rain, cicadas, irregular drips. The generated version is smoother, more \"musical.\" This gap is exactly what models like AudioLDM and Stable Audio are trying to close.</p>"},{"location":"courses/hf-audio/chapter2.html#the-full-loop-audio-text-audio","title":"The full loop: Audio \u2192 Text \u2192 Audio","text":"<p>Here's something fun. Take a recording, caption it, then generate new audio from that caption. A full Audio \u2192 Text \u2192 Audio round trip.</p> <pre><code>clip = thunder[:SR * 30]\ncaption = caption_audio(clip, SR, style=\"audiocaps &gt; caption: \")\n# \u2192 \"Rain falling and th falling\"\noutput = music_pipe(caption, forward_params={\"max_new_tokens\": 512})\n</code></pre> <p>Original recording: </p> <p>Caption generated: \"Rain falling and th falling\"</p> <p>Regenerated from caption: </p> <p>Information is lost at each step. The caption doesn't capture everything in the original, and MusicGen interprets the caption in its own way. Still, it's remarkable that this works at all with pre trained models and zero custom training.</p>"},{"location":"courses/hf-audio/chapter2.html#what-i-learned","title":"What I learned","text":"<p>Three <code>pipeline()</code> tasks, zero training, and we got surprisingly far:</p> Task Model What it does Audio Classification <code>MIT/ast-finetuned-audioset</code> Audio \u2192 label (e.g. \"rain\", \"chime\") Audio Captioning <code>MU-NLPC/whisper-tiny-audio-captioning</code> Audio \u2192 free text description Audio Generation <code>facebook/musicgen-small</code> Text \u2192 audio <p><code>pipeline()</code> abstracts away all preprocessing. You give it raw audio, it gives you results. When there's no pipeline wrapper (like the captioning model), you load model, feature extractor, and tokenizer separately. Pre trained models get you surprisingly far, but they have blind spots. Fine tuning on your specific data (coming in later chapters) is what fixes that. And you can chain tasks together (caption \u2192 generate) for creative applications, even if information is lost at each step.</p> <p>\ud83d\udcd3 Full notebook with all the code</p>"},{"location":"courses/hf-audio/chapter3.html","title":"Chapter 3: Inside Audio Transformers","text":"<p> Following the Hugging Face Audio Course, Unit 3. The course chapter is pure theory: CTC vs Seq2Seq, encoder only vs encoder decoder, how audio enters a transformer. All essential, but no code. This notebook makes it tangible. We load Wav2Vec2, Whisper, and AST side by side, trace the data flow through each, and visualize the attention patterns on our thunder and chimes recordings. </p> <p>\ud83d\udcd3 Full notebook on GitHub</p>"},{"location":"courses/hf-audio/chapter3.html#part-1-three-architectures-one-audio-clip","title":"Part 1: Three Architectures, One Audio Clip","text":"<p>The HF course describes three architecture families for audio:</p> Architecture Type Input Example Used for CTC Encoder only Raw waveform Wav2Vec2 ASR (speech to characters) Seq2Seq Encoder decoder Mel spectrogram Whisper ASR, captioning, translation Classification Encoder only Spectrogram patches AST Audio classification <p>We load all three and feed them the same 5 second clips of thunder and chimes.</p> <pre><code>from transformers import (\n    Wav2Vec2Model, Wav2Vec2FeatureExtractor,\n    WhisperModel, WhisperFeatureExtractor,\n    ASTModel, AutoFeatureExtractor,\n)\n\nw2v_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\", attn_implementation=\"eager\")\nwhisper_model = WhisperModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")\nast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", attn_implementation=\"eager\")\n</code></pre> <p>We need <code>attn_implementation=\"eager\"</code> to extract attention weights. The default <code>\"sdpa\"</code> is faster but doesn't support <code>output_attentions=True</code>.</p>"},{"location":"courses/hf-audio/chapter3.html#parameter-counts","title":"Parameter counts","text":"Model Parameters Type Encoder layers Decoder layers Wav2Vec2 base 94.4M Encoder only 12 n/a Whisper tiny 37.8M Encoder decoder 4 4 AST 86.2M Encoder only n/a (ViT) n/a"},{"location":"courses/hf-audio/chapter3.html#how-each-model-sees-the-same-audio","title":"How each model sees the same audio","text":"<p>Each model preprocesses audio completely differently:</p> Model Input format Shape (5s clip) Wav2Vec2 Raw waveform (1, 80000) Whisper Mel spectrogram (1, 80, 3000) AST Spectrogram patches (1, 1024, 128) <p>Wav2Vec2 eats the raw samples directly. Whisper converts to a mel spectrogram first. AST chops the spectrogram into patches like a Vision Transformer. Same audio, three completely different representations before the model even starts.</p>"},{"location":"courses/hf-audio/chapter3.html#what-comes-out-hidden-states","title":"What comes out: hidden states","text":"Model Output shape Meaning Wav2Vec2 (1, 249, 768) 249 time steps \u00d7 768d, one vector per ~20ms Whisper (1, 1500, 384) 1500 time steps \u00d7 384d, the decoder cross attends to this AST (1, 1214, 768) 1214 patches \u00d7 768d, pooled for classification"},{"location":"courses/hf-audio/chapter3.html#hidden-states-as-heatmaps","title":"Hidden states as heatmaps","text":"<p>Thunder vs chimes, across all three architectures. We're looking at the first 128 dimensions of each model's output:</p> <p></p> <p>Each model creates a completely different representation of the same audio. Wav2Vec2 is fine grained with many time steps, learned directly from the raw waveform. Whisper has coarser time resolution, derived from the mel spectrogram. AST is patch based, like a vision transformer looking at the spectrogram as an image. And the same model produces visibly different patterns for thunder vs chimes.</p>"},{"location":"courses/hf-audio/chapter3.html#part-2-inside-whisper-encoder-vs-decoder","title":"Part 2: Inside Whisper, Encoder vs Decoder","text":"<p>Whisper is the most interesting to explore because it has both an encoder and a decoder. The full data flow:</p> <pre><code>Raw audio (80,000 samples)\n  \u2192 Mel spectrogram (1, 80, 3000)  \u2190 always padded to 30s\n    \u2192 Encoder \u2192 (1, 1500, 384)     \u2190 3000 frames / 2 from conv layers\n      \u2192 Decoder cross-attends to encoder output\n        \u2192 generates tokens one at a time\n</code></pre> <pre><code>print(\"Whisper-tiny architecture:\")\n# Encoder: 4 layers, 6 heads, 384d\n# Decoder: 4 layers, 6 heads, 384d\n# Vocab size: 51865\n# Max source positions: 1500 (= 30s of audio)\n# Max target positions: 448 (= max tokens to generate)\n</code></pre> <p>The encoder compresses 3000 mel frames down to 1500 time steps via two convolutional layers (stride 2). Each output step is a 384 dimensional vector representing about 20ms of audio. The decoder then cross attends to these 1500 encoder states when generating each output token.</p>"},{"location":"courses/hf-audio/chapter3.html#part-3-visualizing-attention","title":"Part 3: Visualizing Attention","text":"<p>This is the payoff. Two types of attention to look at: encoder self attention (how the model connects different parts of the audio) and cross attention (how the decoder \"looks at\" the audio when generating each token).</p>"},{"location":"courses/hf-audio/chapter3.html#encoder-self-attention","title":"Encoder self attention","text":"<p>We extract attention from all 4 encoder layers and visualize the first 200 time steps (~2.6 seconds):</p> <p></p> <p>The pattern is clear: early layers attend mostly locally (the bright diagonal), while deeper layers develop broader, more global attention patterns. This makes sense. Low level features like frequency content are local, but higher level concepts like \"this is rain\" or \"this is a chime strike\" require connecting distant parts of the signal.</p>"},{"location":"courses/hf-audio/chapter3.html#individual-attention-heads","title":"Individual attention heads","text":"<p>Each head in the last encoder layer learns a different pattern:</p> <p></p> <p>Some heads focus on local context (sharp diagonal), others attend broadly. Some seem to specialize in periodic patterns. This is the model distributing different aspects of audio understanding across its heads.</p>"},{"location":"courses/hf-audio/chapter3.html#cross-attention-where-the-decoder-looks","title":"Cross attention: where the decoder looks","text":"<p>When Whisper tries to \"transcribe\" our environmental sounds (it's a speech model, so it doesn't know what to do with rain and chimes), the cross attention shows where the decoder looks in the audio for each generated token.</p> <pre><code>thunder_ca, thunder_tokens, thunder_text = get_cross_attention(thunder_10s)\nchimes_ca, chimes_tokens, chimes_text = get_cross_attention(chimes_10s)\n\n# Thunder: Whisper heard '.'\n# Chimes: Whisper heard ' you'\n</code></pre> <p>Whisper generates almost nothing for both clips, just a period for thunder and \"you\" for chimes. It's a speech model confronted with non speech audio. But the cross attention is still revealing:</p> <p></p> <p>Even when Whisper has nothing meaningful to transcribe, the attention tells us which parts of the audio the model found \"interesting.\" The special tokens (startoftranscript, en, transcribe, notimestamps) each attend to different parts of the audio as the model tries to figure out what it's hearing.</p>"},{"location":"courses/hf-audio/chapter3.html#attention-over-the-spectrogram","title":"Attention over the spectrogram","text":"<p>Here we overlay the total cross attention on top of the mel spectrogram for each sound:</p> <p></p> <p>Thunder/rain has continuous energy spread across the clip, so attention is distributed broadly. Chimes have distinct transient events, and the attention profile reflects this. Whisper is a speech model trying to process environmental sounds, but the attention patterns still tell us what the model finds acoustically salient.</p>"},{"location":"courses/hf-audio/chapter3.html#what-i-learned","title":"What I learned","text":"What we explored What we learned 3 architectures (Wav2Vec2, Whisper, AST) Same audio, completely different representations Input formats Raw waveform vs mel spectrogram vs spectrogram patches Encoder self attention Early layers = local, deeper layers = global Attention heads Each head specializes in a different pattern Cross attention Where the decoder \"looks\" when generating each token Attention + spectrogram Links attention to actual acoustic events <p>The big takeaway: these models build up increasingly abstract representations of audio through their layers. The attention visualizations make the theory from the HF course concrete. You can actually see the model learning to connect distant parts of a rain recording, or focusing on the transient attacks of chimes.</p> <p>\ud83d\udcd3 Full notebook with all the code</p>"},{"location":"courses/hf-audio/chapter4.html","title":"Chapter 4: Fine Tuning Audio Classifiers","text":"<p> Following the Hugging Face Audio Course, Unit 4. The course fine tunes DistilHuBERT on GTZAN for music genre classification. I do the same, then go further: train my own environmental sound classifier on ESC 50 and test it on our thunder and chimes field recordings. Same architecture, same pipeline, completely different tasks. Swap the dataset and labels, everything else stays the same. </p> <p>\ud83d\udcd3 Full notebook on GitHub</p>"},{"location":"courses/hf-audio/chapter4.html#try-it-yourself","title":"Try it yourself","text":"<p>I deployed the ESC 50 classifier as a Gradio demo on Hugging Face Spaces. Record or upload any sound and see what the model thinks it is:</p> <p> </p>"},{"location":"courses/hf-audio/chapter4.html#part-a-music-genre-classification-gtzan","title":"Part A: Music Genre Classification (GTZAN)","text":"<p>GTZAN is 999 songs, each 30 seconds, across 10 genres: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock. The HF course fine tunes DistilHuBERT on this. GTZAN's 30 second clips eat too much RAM for Colab free tier during preprocessing, so we load a pretrained checkpoint and test it on our own music.</p> <pre><code>from transformers import pipeline\n\npipe_gtzan = pipeline(\n    \"audio-classification\",\n    model=\"sanchit-gandhi/distilhubert-finetuned-gtzan\",\n)\n# 23.7M parameters, 10 genre labels\n</code></pre>"},{"location":"courses/hf-audio/chapter4.html#classify-our-music","title":"Classify our music","text":"<p>I tested Bach (classical harpsichord, ~20 minutes) and a jazz recording (RainbowJazz, ~6 minutes). For each, I classify 5 second segments at the beginning, middle, and end.</p> <p>Bach gets classified as... hiphop (76.6% at the beginning). That's the model confidently being wrong. The harpsichord's rhythmic patterns apparently look like hip hop to DistilHuBERT. Some segments do get \"classical\" but it's inconsistent. The jazz recording does better, getting \"jazz\" more reliably.</p>"},{"location":"courses/hf-audio/chapter4.html#genre-predictions-over-time","title":"Genre predictions over time","text":"<p>Sliding a 5 second window across the first 60 seconds of each recording:</p> <p></p> <p>The predictions jump around. Bach flips between hiphop, blues, and classical depending on the segment. This tells us something important: the model is sensitive to local texture, not long range structure. A 5 second window of harpsichord arpeggios might genuinely have rhythmic features that overlap with other genres.</p>"},{"location":"courses/hf-audio/chapter4.html#part-b-environmental-sound-classification-esc-50","title":"Part B: Environmental Sound Classification (ESC 50)","text":"<p>Now we train our own classifier from scratch on Colab. ESC 50 has 2000 clips (5 seconds each) across 50 categories:</p> Group Examples Animals dog, rooster, pig, cow, frog, cat, hen, insects, sheep, crow Natural soundscapes rain, sea waves, crackling fire, crickets, chirping birds, water drops, wind, thunderstorm Human (non speech) crying baby, sneezing, clapping, breathing, coughing, footsteps, laughing Interior/domestic door knock, mouse click, keyboard, washing machine, vacuum cleaner, clock alarm Exterior/urban helicopter, chainsaw, siren, car horn, engine, train, church bells, airplane <p>Perfectly balanced: 40 clips per category. 5 second clips fit in Colab free tier.</p>"},{"location":"courses/hf-audio/chapter4.html#prepare-data","title":"Prepare data","text":"<p>ESC 50 comes with 5 predefined folds. Folds 1 through 4 for training (1600 clips), fold 5 for testing (400 clips).</p> <pre><code>from transformers import AutoFeatureExtractor\n\nmodel_id = \"ntu-spml/distilhubert\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\n    model_id, do_normalize=True, return_attention_mask=True\n)\n\n# Resample to 16kHz, preprocess\nesc_train = esc_train.cast_column(\"audio\", HFAudio(sampling_rate=16000))\n\ndef preprocess_esc(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays, sampling_rate=16000,\n        max_length=int(16000 * 5.0), truncation=True,\n        return_attention_mask=True,\n    )\n    return inputs\n\nesc_train_encoded = esc_train.map(preprocess_esc, batched=True)\n</code></pre> <p>Each input: 80,000 samples (5.0s at 16 kHz), normalized to mean ~0 and variance ~1.</p>"},{"location":"courses/hf-audio/chapter4.html#fine-tune-distilhubert","title":"Fine tune DistilHuBERT","text":"<p>Same 23.7M parameter model as GTZAN, just with 50 output labels instead of 10.</p> <pre><code>from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n\nmodel_esc = AutoModelForAudioClassification.from_pretrained(\n    \"ntu-spml/distilhubert\",\n    num_labels=50,\n    label2id=esc_label2id,\n    id2label=esc_id2label,\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"distilhubert-finetuned-esc50\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=5,\n    warmup_ratio=0.1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n)\n</code></pre> <p>Training on Colab free tier (CPU, no GPU). 5 epochs, ~9 hours. Here's the training log:</p> Epoch Training Loss Validation Loss Accuracy 1 3.610 3.539 16.8% 2 3.221 3.175 26.5% 3 3.014 2.938 38.5% 4 2.771 2.807 42.8% 5 2.676 2.758 46.8% <p>46.8% accuracy on 50 classes. Random chance would be 2%, so the model is learning a lot. But 50 classes with only 40 examples each is tough. More epochs and GPU training would help.</p>"},{"location":"courses/hf-audio/chapter4.html#evaluate","title":"Evaluate","text":"<p>The confusion matrix shows where the model gets confused:</p> <p></p> <p>Best and worst categories:</p> Easiest (100%) Hardest (0%) pouring_water can_opening sea_waves car_horn coughing engine <p>Some categories with strong, distinctive spectral signatures (sea waves, pouring water) are easy. Short transient sounds (can opening, car horn) and sounds that overlap acoustically with other categories are the hardest.</p>"},{"location":"courses/hf-audio/chapter4.html#part-c-test-on-our-soundscape-recordings","title":"Part C: Test on Our Soundscape Recordings","text":"<p>The real test: does our ESC 50 model recognize thunder and chimes from our own field recordings? These are real world recordings, not clean 5 second clips.</p> Sound Top 1 Score Top 2 Score Top 3 Score Thunder (Ubud) sea_waves 7.5% washing_machine 6.7% train 6.7% Chimes (Georgia) sneezing 8.5% cat 7.4% rooster 7.3% <p>Not great. The model is confused because our recordings don't sound like the clean 5 second ESC 50 clips it was trained on. The thunder recording has layered rain, cicadas, and dripping, not just clean thunder. The chimes have silence between strikes, which the model hasn't learned to handle.</p>"},{"location":"courses/hf-audio/chapter4.html#sliding-window-across-the-full-recordings","title":"Sliding window across the full recordings","text":"<p>What happens when we slide a 5 second window across the entire recording?</p> <p></p> <p>The predictions jump around as the acoustic content changes. Thunder with heavy rain might get \"rain\" or \"thunderstorm.\" During quiet moments it might flip to something unrelated. This is exactly what you'd expect from a model trained on isolated 5 second clips being tested on long, complex field recordings.</p>"},{"location":"courses/hf-audio/chapter4.html#what-i-learned","title":"What I learned","text":"GTZAN (Music) ESC 50 (Environment) Task Genre classification Sound classification Classes 10 genres 50 categories Data 999 songs, 30s each 2000 clips, 5s each Base model DistilHuBERT DistilHuBERT Tested on Bach, RainbowJazz Thunder, Chimes <p>Training data defines the model. The architecture is just the vessel. Same 23.7M parameter model, completely different capabilities depending on what you fine tune it on. Transfer learning works: a speech model (HuBERT, trained on LibriSpeech) adapts to music genres and environmental sounds. 50 classes is harder than 10, obviously, but even with limited data and CPU training you get meaningful results. And the sliding window analysis shows how predictions change as acoustic content evolves, which is something you don't see when you only test on clean isolated clips.</p> <p>\ud83d\udcd3 Full notebook with all the code</p>"}]}