#  How to Learn AI for Audio & Sound

<p class="hero-subtitle">
Ciao! This is My Sonicase blog on how I learn AI for sound and audio. I believe the best way to actually learn AI is to build things. Follow a great course, and in the meantime go implement stuff. Read a paper, then try to reproduce it. Break things. That's how it sticks. So this is my open lab where I do exactly that. The twist is that <strong>everything I build, I apply to sound, music, and audio</strong>. Even when a course teaches general deep learning, I always steer my projects toward generative soundscapes, procedural audio, and creative sonic experiments. If it makes a sound, I'm happier and learn better. Let's go!
</p>

<div class="keywords">
  <span>generative audio</span>
  <span>generative soundscapes</span>
  <span>procedural AI sound</span>
  <span>music generation</span>
  <span>gen AI</span>
  <span>audio synthesis</span>
  <span>neural audio</span>
  <span>sound design  ML</span>
</div>

---

Each tile links to a page where I document what I'm learning, the code I write, and the audio experiments I run. It's all a work in progress. That's the point.

<p class="section-label"> Foundational AI</p>

<div class="grid-cards">

<a class="card foundational" href="courses/karpathy/">
  <span class="card-icon">И</span>
  <span class="tag foundational">Foundational</span>
  <h3>Karpathy 路 Neural Networks: Zero to Hero</h3>
  <p>Build GPT from scratch, character by character. The concepts (tokenization, embeddings, self attention, autoregressive generation) are exactly what powers MusicGen and AudioLM. Where I can, I swap text for audio tokens.</p>
</a>

<a class="card foundational" href="courses/fastai/">
  <span class="card-icon"></span>
  <span class="tag foundational">Foundational</span>
  <h3>fast.ai 路 Practical Deep Learning</h3>
  <p>Top down, code first deep learning. I take every lesson and rebuild the exercises around audio spectrograms, waveform classification, and sound generation.</p>
</a>

<a class="card foundational" href="courses/deeplearning-ai/">
  <span class="card-icon"></span>
  <span class="tag foundational">Foundational</span>
  <h3>Andrew Ng 路 Deep Learning Specialization</h3>
  <p>The classic. Neural networks from scratch, CNNs, sequence models, all re applied to audio features, mel spectrograms, and music related tasks.</p>
</a>

</div>

<p class="section-label"> Applied AI for Audio</p>

<div class="grid-cards">

<a class="card applied" href="courses/hf-audio/">
  <span class="card-icon"></span>
  <span class="tag applied">Applied 路 Audio</span>
  <h3>Hugging Face Audio Course</h3>
  <p>Hands on transformers for audio. Classification, generation, music tagging, working with audio datasets on the Hub.</p>
</a>

</div>

<p class="section-label"> Generative & Creative AI</p>

<div class="grid-cards">

<a class="card generative" href="courses/gen-audio/">
  <span class="card-icon"></span>
  <span class="tag generative">Generative</span>
  <h3>Generative Audio Experiments</h3>
  <p>My sandbox for diffusion based sound, neural synthesis, generative soundscapes, and procedural audio. Projects I want to try, ideas I want to test.</p>
</a>

</div>

<p class="section-label"> Papers & Research</p>

<div class="grid-cards">

<a class="card research" href="papers/">
  <span class="card-icon"></span>
  <span class="tag research">Research</span>
  <h3>Paper Implementations</h3>
  <p>Key papers I study and reproduce: AudioLDM, MusicGen, Riffusion, RAVE, SampleRNN, and others. Notes and code for each.</p>
</a>

</div>

---

!!! tip "How to navigate"
    Click any tile to see my notes, code, and audio demos. Everything here is unfinished and evolving. That's how learning works.
